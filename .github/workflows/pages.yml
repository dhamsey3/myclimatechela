name: Build & Deploy (static + Medium)

on:
  push:
    branches:
      - main
      - master
  schedule:
    - cron: '0 */6 * * *'
  workflow_dispatch:

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: pagesname: Build & Deploy (static + Medium)

on:
  push:
    branches: [ main, master ]
  schedule:
    - cron: '0 */6 * * *'
  workflow_dispatch:

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: pages
  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install feedparser requests

      - name: Generate posts.json from Medium RSS
        run: |
          python - << 'PY'
          import re, html, textwrap, time, datetime, json, pathlib, sys
          import feedparser, requests

          FEED = "https://medium.com/feed/@myclimatedefinition"  # <-- change to your handle
          out = pathlib.Path("public/posts.json")
          out.parent.mkdir(parents=True, exist_ok=True)

          def clean(s): return html.unescape(re.sub("<.*?>","", s or ""))
          def to_iso(ts):
              if not ts: return ""
              try:
                  return datetime.datetime.fromtimestamp(time.mktime(ts), datetime.timezone.utc).isoformat()
              except Exception:
                  return ""

          def extract_image(entry):
              if entry.get("media_thumbnail"):
                  u = entry["media_thumbnail"][0].get("url")
                  if u: return u
              if entry.get("media_content"):
                  u = entry["media_content"][0].get("url")
                  if u: return u
              blob = entry.get("summary","")
              for c in entry.get("content", []):
                  blob += " " + c.get("value","")
              m = re.search(r'<img[^>]+src=["\']([^"\']+)["\']', blob, re.I)
              return m.group(1) if m else None

          headers = {"User-Agent": "Mozilla/5.0"}
          items = []
          try:
              r = requests.get(FEED, headers=headers, timeout=30)
              r.raise_for_status()
              feed = feedparser.parse(r.content)
              for e in getattr(feed, "entries", []):
                  items.append({
                      "title": e.get("title","Untitled"),
                      "permalink": e.get("link","#"),
                      "external_url": e.get("link","#"),
                      "date": to_iso(e.get("published_parsed")),
                      "summary": textwrap.shorten(clean(e.get("summary","")), 220),
                      "image": extract_image(e),
                  })
          except Exception as e:
              print(f"ERROR fetching feed: {e}", file=sys.stderr)

          out.write_text(json.dumps(items, ensure_ascii=False), encoding="utf-8")
          print(f"Wrote {len(items)} posts to {out}")
          PY

      - name: Verify required files exist
        run: |
          test -f public/index.html || (echo "public/index.html missing!" && exit 1)
          test -f public/css/style.css || (echo "public/css/style.css missing!" && exit 1)
          test -f public/js/main.js || (echo "public/js/main.js missing!" && exit 1)
          test -f public/posts.json || (echo "public/posts.json missing!" && exit 1)

      - name: List files to deploy
        run: ls -lah public && echo "----" && find public -maxdepth 2 -type f -print

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: public

  deploy:
    needs: build
    runs-on: ubuntu-latest
    steps:
      - uses: actions/deploy-pages@v4

  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install feedparser requests

      - name: Generate posts.json from Medium RSS
        run: |
          python - << 'PY'
          import re, html, textwrap, time, datetime, json, pathlib, sys
          import feedparser, requests

          FEED = "https://medium.com/feed/@myclimatedefinition"  # <-- change to your handle
          out = pathlib.Path("public/posts.json")
          out.parent.mkdir(parents=True, exist_ok=True)

          def clean(s): return html.unescape(re.sub("<.*?>","", s or ""))
          def to_iso(ts):
              if not ts: return ""
              try:
                  return datetime.datetime.fromtimestamp(time.mktime(ts), datetime.timezone.utc).isoformat()
              except Exception:
                  return ""

          def extract_image(entry):
              if entry.get("media_thumbnail"):
                  u = entry["media_thumbnail"][0].get("url")
                  if u: return u
              if entry.get("media_content"):
                  u = entry["media_content"][0].get("url")
                  if u: return u
              blob = entry.get("summary","")
              for c in entry.get("content", []):
                  blob += " " + c.get("value","")
              m = re.search(r'<img[^>]+src=["\']([^"\']+)["\']', blob, re.I)
              return m.group(1) if m else None

          headers = {"User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/124 Safari/537.36"}
          try:
              r = requests.get(FEED, headers=headers, timeout=30)
              r.raise_for_status()
              feed = feedparser.parse(r.content)
          except Exception as e:
              print(f"ERROR fetching feed: {e}", file=sys.stderr)
              feed = feedparser.parse(b"")

          items = []
          for e in feed.entries:
              items.append({
                  "title": e.get("title","Untitled"),
                  "permalink": e.get("link","#"),
                  "external_url": e.get("link","#"),
                  "date": to_iso(e.get("published_parsed")),
                  "summary": textwrap.shorten(clean(e.get("summary","")), 220),
                  "image": extract_image(e),
              })

          out.write_text(json.dumps(items, ensure_ascii=False), encoding="utf-8")
          print(f"Wrote {len(items)} posts to {out}")
          PY

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: public

  deploy:
    needs: build
    runs-on: ubuntu-latest
    steps:
      - uses: actions/deploy-pages@v4
